{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHrMnzdAV-_c"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from collections import defaultdict\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.parse import DependencyGraph\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('dependency_treebank')\n",
    "\n",
    "\n",
    "# Adding digits, lowercase letters to stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([str(i) for i in range(10)])  # Adding digits\n",
    "stop_words.update(list(string.ascii_lowercase))  # Adding individual lowercase letters\n",
    "\n",
    "# Add stop words from stoplist.txt\n",
    "with open('stoplist.txt', 'r') as f:\n",
    "    stoplist_words = f.read().splitlines()\n",
    "stop_words.update(stoplist_words)\n",
    "\n",
    "def pretagging(txt):\n",
    "    rules = [\n",
    "        # Remove commas inside numbers\n",
    "        [r'(\\d),(\\d)', r'\\1\\2'],\n",
    "        # Replace periods inside web domains with <dot>\n",
    "        [r'(\\w)\\.(\\w)', r'\\1<dot>\\2'],\n",
    "        # Insert white space in front of a unit where necessary\n",
    "        [r'(\\d)([a-z]+)', r'\\1 \\2'],\n",
    "        # Compress repetitive punctuation into a single character\n",
    "        [r'([!?.-])\\1+', r'\\1'],\n",
    "        # Normalize whitespace\n",
    "        [r'\\s+', ' '],\n",
    "        # Normalize non-ASCII characters\n",
    "        [r'[^\\x00-\\x7F]+', ' ']\n",
    "    ]\n",
    "    # Apply all rules\n",
    "    for pattern, replacement in rules:\n",
    "        txt = re.sub(pattern, replacement, txt)\n",
    "    return txt\n",
    "\n",
    "def hyphen(txt):\n",
    "    txt = re.sub('([a-z])\\\\-([a-z])', '\\\\1 \\\\2', txt, flags=re.IGNORECASE)\n",
    "    txt = re.sub('([a-z])\\\\-([a-z])', '\\\\1 \\\\2', txt, flags=re.IGNORECASE)\n",
    "    return txt\n",
    "\n",
    "def process_text(text):\n",
    "    # Apply pre-processing\n",
    "    text = pretagging(text)\n",
    "    text = hyphen(text)\n",
    "    # Remove punctuation and convert to lower case\n",
    "    text = re.sub(r'[^\\w\\s<dot>]', ' ', text.lower())\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Replace <dot> with . in tokens\n",
    "    tokens = [token.replace('<dot>', '.') if '<dot>' in token else token for token in tokens]\n",
    "    # Filter out stop words and tokens containing 'dot'\n",
    "    tokens = [token for token in tokens if token not in stop_words and 'dot' not in token and len(token) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def process_dataset(filename):\n",
    "    bigram_freq = FreqDist()\n",
    "    first_word_freq = defaultdict(int)\n",
    "    second_word_freq = defaultdict(int)\n",
    "    bigram_pos = defaultdict(str)  # For storing POS tags of bigrams\n",
    "    total_words = 0\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            tokens = process_text(line)\n",
    "            if tokens:\n",
    "                bigrams = list(ngrams(tokens, 2))\n",
    "                bigram_freq_line = FreqDist(bigrams)\n",
    "                bigram_freq.update(bigram_freq_line)\n",
    "                total_words += len(tokens)\n",
    "\n",
    "                for bigram, freq in bigram_freq_line.items():\n",
    "                    first_word, second_word = bigram\n",
    "                    first_word_freq[first_word] += freq\n",
    "                    second_word_freq[second_word] += freq\n",
    "                    tagged_bigram = nltk.pos_tag(bigram)  # Perform POS tagging\n",
    "                    # Override POS tag for 'data' and 'web'\n",
    "                    tagged_bigram = [(word, 'NN') if word in ['data', 'web'] else (word, tag) for word, tag in tagged_bigram]\n",
    "                    bigram_pos[bigram] = tagged_bigram\n",
    "\n",
    "\n",
    "    # Filter out bigrams with frequency less than 50\n",
    "    bigram_freq = {bigram: freq for bigram, freq in bigram_freq.items() if freq >= 50}\n",
    "\n",
    "    return bigram_freq, first_word_freq, second_word_freq, bigram_pos, total_words\n",
    "\n",
    "bigram_freq, first_word_freq, second_word_freq, bigram_pos, total_words = process_dataset(\"AI_5_only_english.txt\")\n",
    "print(bigram_freq)\n",
    "# Remove duplicates\n",
    "unique_bigrams = set(bigram_freq.keys())\n",
    "\n",
    "# POS tagging and filter bigrams based on patterns\n",
    "selected_bigrams = []\n",
    "for bigram in unique_bigrams:\n",
    "    tagged_bigram = bigram_pos[bigram]  # Retrieve the POS tag from the dictionary\n",
    "    if (\n",
    "        (tagged_bigram[0][1] == 'NN' and tagged_bigram[1][1] == 'NN') or  # Pattern 1\n",
    "        (tagged_bigram[0][1] == 'NN' and tagged_bigram[1][1] == 'NNS') or  # Pattern 2\n",
    "        (tagged_bigram[0][1] == 'JJ' and tagged_bigram[1][1] == 'NN') or  # Pattern 3\n",
    "        (tagged_bigram[0][1] == 'JJ' and tagged_bigram[1][1] == 'NNS')  # Pattern 4\n",
    "    ):\n",
    "        selected_bigrams.append((bigram, tagged_bigram))  # Store the bigram and its POS tag\n",
    "\n",
    "# Rank bigrams\n",
    "ranked_bigrams = sorted(selected_bigrams)\n",
    "\n",
    "# Write the output to a CSV file\n",
    "with open('output4.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Bigram\", \"POS Tag\", \"Frequency\", \"First Word Frequency\", \"Second Word Frequency\"])\n",
    "    for bigram, tagged_bigram in ranked_bigrams:\n",
    "        writer.writerow([bigram, tagged_bigram, bigram_freq[bigram], first_word_freq[bigram[0]], second_word_freq[bigram[1]]])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
