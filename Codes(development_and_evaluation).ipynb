{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9CoovU9y6koGcTFQDm6ye"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#Connecting to the drive\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount = True)"],"metadata":{"id":"xyVl62fkm45E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The original DBLP Discovery Dataset (D3) was downloaded from https://github.com/jpwahle/lrec22-d3-dataset/blob/main/README.md using this link: https://zenodo.org/records/7071698"],"metadata":{"id":"q0NbeICJlBdS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCAhFTUclAVJ"},"outputs":[],"source":["import gzip\n","import shutil\n","\n","# File paths\n","input_file_path = '/content/drive/My Drive/Experiment/2022-11-30-papers.jsonl.gz'\n","output_file_path = '/content/drive/My Drive/Experiment/2022-11-30-papers.jsonl'\n","\n","# Unpack the .gz file\n","with gzip.open(input_file_path, 'rb') as gz_file:\n","    with open(output_file_path, 'wb') as out_file:\n","        shutil.copyfileobj(gz_file, out_file)\n","\n","print(f\"File has been unpacked and saved as {output_file_path}\")"]},{"cell_type":"markdown","source":["Titles and abstracts were retrieved and preprocessed"],"metadata":{"id":"_UznDwjfmpTf"}},{"cell_type":"code","source":["import json\n","import nltk\n","from nltk.tokenize import word_tokenize\n","\n","# Download necessary NLTK resources\n","nltk.download('punkt')\n","\n","# Function to preprocess the text\n","def preprocess(txt):\n","    # Lowercase and tokenize the text\n","    return word_tokenize(txt.lower())\n","\n","# Paths to input and output files\n","input_file_path = '/content/drive/My Drive/Experiment/2022-11-30-papers.jsonl'\n","output_file_path = '/content/drive/My Drive/Experiment/corpus/titles_and_abstracts.txt'\n","chunk_size = 10000  # Number of lines to process in each chunk\n","\n","# First, calculate the total number of lines for progress tracking\n","with open(input_file_path, 'r') as f:\n","    total_lines = sum(1 for line in f)\n","\n","# Open the output file for writing\n","try:\n","    with open(output_file_path, 'w') as output_file:\n","        # Read and process the input JSONL file in chunks\n","        with open(input_file_path, 'r') as input_file:\n","            for start_line in range(0, total_lines, chunk_size):\n","                # Process the chunk\n","                lines = [input_file.readline() for _ in range(chunk_size)]\n","                for line_number, line in enumerate(lines, start=start_line + 1):\n","                    try:\n","                        # Parse the JSON line\n","                        data = json.loads(line.strip())\n","\n","                        # Extract and preprocess title and abstract\n","                        title = data.get('title', '').strip()\n","                        abstract = data.get('abstract', '').strip()\n","\n","                        # Use the preprocess function to tokenize title and abstract\n","                        processed_title = ' '.join(preprocess(title))\n","                        processed_abstract = ' '.join(preprocess(abstract))\n","\n","                        # Write the processed title and abstract to the output file, only if they are not empty\n","                        if processed_title:\n","                            output_file.write(processed_title + \" \")\n","                        if processed_abstract:\n","                            output_file.write(processed_abstract + \" \")\n","\n","                    except json.JSONDecodeError:\n","                        print(f\"Error decoding JSON on line {line_number}: {line.strip()}\")\n","                        continue  # Skip the line if there is a JSON error\n","\n","                # Display progress percentage\n","                progress = min((start_line + chunk_size) / total_lines * 100, 100)\n","                print(f\"Processing: {progress:.2f}% complete\")\n","\n","    print(f\"Filtered and processed titles and abstracts have been saved to {output_file_path}\")\n","\n","except Exception as e:\n","    print(f\"An error occurred while writing to the file: {e}\")"],"metadata":{"id":"5b27Y2Erm1Sj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bigrams were retrieved (to form a list of hypernyms)"],"metadata":{"id":"2Idqx7MDnAEV"}},{"cell_type":"code","source":["import re\n","import math\n","import pandas as pd\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","from nltk.util import bigrams\n","from nltk.corpus import stopwords\n","from nltk import download\n","\n","# Download required NLTK resources\n","download('punkt')\n","download('stopwords')\n","\n","# Define constants\n","MIN_FREQUENCY = 50\n","MIN_LOG_DICE = 6\n","CHUNK_SIZE = 10 * 1024 * 1024  # Process in 10MB chunks\n","\n","# Function to check if a word contains only valid characters (English/Greek letters, digits, dashes, and underscores)\n","def contains_invalid_characters(word):\n","    if not isinstance(word, str):\n","        return True\n","    for char in word:\n","        if not (char.isalpha() and (('A' <= char <= 'Z') or ('a' <= char <= 'z') or ('\\u0370' <= char <= '\\u03FF')) or char in ['-', '_'] or char.isdigit()):\n","            return True\n","    return False\n","\n","# Function to check if the word consists only of numbers or symbols\n","def is_only_numbers_or_symbols(word):\n","    word_cleaned = word.replace(\"-\", \"\").replace(\"_\", \"\")\n","    return word_cleaned.isdigit() or all(not c.isalnum() for c in word_cleaned)\n","\n","# Function to check if the word starts or ends with a dash or underscore\n","def starts_or_ends_with_dash_or_underscore(word):\n","    return word.startswith('-') or word.startswith('_') or word.endswith('-') or word.endswith('_')\n","\n","# Set file paths\n","file_path = '/content/drive/My Drive/Experiment/corpus/titles_and_abstracts.txt'\n","output_file_path = '/content/drive/My Drive/Experiment/filtered_bigrams.csv'\n","\n","# Initialize stopwords and counters\n","stop_words = set(stopwords.words('english'))\n","\n","# Initialize counters for tokens and bigrams\n","token_freq = Counter()\n","bigram_freq = Counter()\n","\n","# Track chunk number\n","chunk_number = 1\n","\n","# Read and process the file in chunks\n","with open(file_path, 'r') as f:\n","    while True:\n","        chunk = f.read(CHUNK_SIZE)  # Read a chunk of text\n","        if not chunk:\n","            break\n","\n","        print(f\"Processing chunk {chunk_number}...\")\n","\n","        # Tokenize the chunk\n","        tokens = word_tokenize(chunk.lower())\n","\n","        # Update token frequencies\n","        token_freq.update(tokens)\n","\n","        # Generate and count bigrams, filtering out stopwords\n","        filtered_bigrams = [\n","            bigram for bigram in bigrams(tokens)\n","            if bigram[0] not in stop_words and bigram[1] not in stop_words\n","        ]\n","        bigram_freq.update(filtered_bigrams)\n","\n","        # Move to the next chunk\n","        chunk_number += 1\n","\n","# Filter and calculate log dice score for valid bigrams\n","all_filtered_bigrams = []\n","for bigram, freq in bigram_freq.items():\n","    if freq >= MIN_FREQUENCY:\n","        first_word, second_word = bigram\n","\n","        # Check that both words in the bigram meet the conditions\n","        if (not contains_invalid_characters(first_word) and not contains_invalid_characters(second_word) and\n","            not is_only_numbers_or_symbols(first_word) and not is_only_numbers_or_symbols(second_word) and\n","            not starts_or_ends_with_dash_or_underscore(first_word) and not starts_or_ends_with_dash_or_underscore(second_word)):\n","\n","            # Calculate log dice score\n","            freq_first_word = token_freq[first_word]\n","            freq_second_word = token_freq[second_word]\n","\n","            if freq_first_word > 0 and freq_second_word > 0:  # Avoid division by zero\n","                log_dice = 14 + math.log2(2 * freq / (freq_first_word + freq_second_word))\n","\n","                # Check if log dice score meets the threshold\n","                if log_dice >= MIN_LOG_DICE:\n","                    all_filtered_bigrams.append({\n","                        'Bigram': f\"{first_word} {second_word}\",\n","                        'Frequency': freq,\n","                        'Log Dice': log_dice\n","                    })\n","\n","# Convert the filtered bigrams to a DataFrame\n","filtered_bigrams_df = pd.DataFrame(all_filtered_bigrams, columns=['Bigram', 'Frequency', 'Log Dice'])\n","\n","# Save the DataFrame to an output file\n","filtered_bigrams_df.to_csv(output_file_path, index=False)\n","\n","print(f\"Filtered bigrams have been saved to {output_file_path}\")"],"metadata":{"id":"pxDmEniWnFI2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sequencies were retrieved (to extract candidate MWTs)"],"metadata":{"id":"fMGhuOeanFbg"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import csv\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from collections import defaultdict\n","\n","# Download NLTK's tokenizer if not already downloaded\n","nltk.download('punkt')\n","\n","# File paths\n","file_path = '/content/drive/My Drive/Experiment/corpus/titles_and_abstracts.txt'\n","control_list_path = '/content/drive/My Drive/Experiment/Hypernym.xlsx'\n","output_path = '/content/drive/My Drive/Experiment/Sequencies.csv'\n","\n","# Load expressions from Excel file\n","control_list_df = pd.read_excel(control_list_path)\n","expressions_to_find = control_list_df['Hypernym'].dropna().tolist()\n","\n","# Prepare expressions in tokenized form for easier matching\n","tokenized_expressions = {\n","    \" \".join(word_tokenize(expression.lower())): expression\n","    for expression in expressions_to_find\n","}\n","\n","# List to store found contexts\n","all_found_contexts = []\n","\n","# Define chunk size: 10MB\n","CHUNK_SIZE = 10 * 1024 * 1024  # 10MB\n","\n","# Process the corpus in chunks\n","chunk_number = 0\n","with open(file_path, 'r') as f:\n","    while True:\n","        # Read the next chunk\n","        chunk = f.read(CHUNK_SIZE)\n","        if not chunk:\n","            break  # End of file\n","\n","        chunk_number += 1\n","        print(f\"Processing chunk {chunk_number}...\")\n","\n","        # Tokenize the chunk\n","        tokens = word_tokenize(chunk.lower())  # Tokenize chunk by chunk\n","        token_positions = defaultdict(list)\n","\n","        # Index each token's position in the chunk\n","        for idx, token in enumerate(tokens):\n","            token_positions[token].append(idx)\n","\n","        # Search for each expression in the tokenized chunk\n","        for tokenized_expression, original_expression in tokenized_expressions.items():\n","            expression_tokens = tokenized_expression.split()\n","            first_token = expression_tokens[0]\n","\n","            # Only proceed if the first token of the expression is in the chunk\n","            if first_token in token_positions:\n","                for j in token_positions[first_token]:\n","                    # Check if the entire expression matches\n","                    if tokens[j:j + len(expression_tokens)] == expression_tokens:\n","                        # Extract context: up to 10 tokens before the expression\n","                        before_context_tokens = tokens[max(0, j - 10):j]\n","                        before_context = \" \".join(before_context_tokens)\n","                        context = f\"{before_context} {tokenized_expression}\".strip()\n","                        all_found_contexts.append((original_expression, context))\n","\n","# Final report of contexts\n","if not all_found_contexts:\n","    print(\"No expressions were found in the text.\")\n","else:\n","    print(f\"Total contexts found: {len(all_found_contexts)}\")\n","\n","# Write results to a CSV file\n","with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow(['Expression', 'Context'])\n","    writer.writerows(all_found_contexts)\n","\n","print(f'Successfully saved contexts to {output_path}')"],"metadata":{"id":"Ah1Fe8-MnP0_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Statistical evaluation of MWTs and Hypernyms (Part I. Absolute frequency)"],"metadata":{"id":"wDk84TxrnYKF"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","import concurrent.futures\n","from concurrent.futures import ProcessPoolExecutor\n","import time\n","\n","# Define paths and load expressions\n","controllist_path = '/content/drive/My Drive/Experiment/full_list.xlsx'\n","text_path = '/content/drive/My Drive/Experiment/corpus/titles_and_abstracts.txt'\n","controllist_df = pd.read_excel(controllist_path)\n","expressions = controllist_df['Terms'].tolist()\n","\n","# Chunk size and batch size\n","CHUNK_SIZE = 10 * 1024 * 1024  # 10MB\n","BATCH_SIZE = 50  # Process 50 chunks at a time\n","\n","# Frequency function\n","def count_occurrences(text, multiword):\n","    pattern = re.escape(multiword)\n","    matches = re.findall(pattern, text)\n","    return len(matches)\n","\n","# Process a single chunk\n","def process_chunk(chunk, expressions):\n","    chunk_frequencies = [count_occurrences(chunk, expr) for expr in expressions]\n","    return chunk_frequencies\n","\n","# Initialize frequency array\n","frequencies = [0] * len(expressions)\n","\n","# Start chunk processing\n","chunk_counter = 0\n","futures = []\n","\n","with open(text_path, 'r', encoding='utf-8') as file, ProcessPoolExecutor() as executor:\n","    while True:\n","        chunk = file.read(CHUNK_SIZE)\n","        if not chunk:\n","            break\n","        chunk_counter += 1\n","        print(f\"Submitting chunk {chunk_counter} for processing...\")\n","        futures.append(executor.submit(process_chunk, chunk, expressions))\n","\n","        # Process futures in batches to reduce memory usage\n","        if len(futures) >= BATCH_SIZE:\n","            print(f\"Processing batch of {BATCH_SIZE} chunks...\")\n","            for future in concurrent.futures.as_completed(futures, timeout=300):\n","                chunk_frequencies = future.result()\n","                for i, freq in enumerate(chunk_frequencies):\n","                    frequencies[i] += freq\n","            futures.clear()  # Clear processed futures\n","\n","# Process remaining futures\n","for future in concurrent.futures.as_completed(futures, timeout=300):\n","    chunk_frequencies = future.result()\n","    for i, freq in enumerate(chunk_frequencies):\n","        frequencies[i] += freq\n","\n","# Add frequencies to DataFrame and save results\n","controllist_df['Absolute Frequency'] = frequencies\n","output_path = '/content/drive/My Drive/Experiment/statistics.xlsx'\n","controllist_df.to_excel(output_path, index=False)\n","\n","print(f\"Results have been saved to {output_path}\")"],"metadata":{"id":"kyr9vufZnn_w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Statistical evaluation of MWTs and Hypernyms (Part II. C-value)"],"metadata":{"id":"h_nINsksniaQ"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Load the data from the Excel file\n","file_path = '/content/drive/My Drive/Experiment/statistics.xlsx'\n","df = pd.read_excel(file_path)\n","\n","# Initialize a new column for C-values\n","df['C-value'] = 0.0\n","\n","# Create a dictionary to store the frequency of each term\n","term_frequencies = dict(zip(df['Terms'], df['Absolute Frequency']))\n","\n","# Function to calculate C-value for a term\n","def calculate_c_value(term, freq_a, candidates):\n","    # Calculate length in terms of words\n","    length_a = len(term.split())  # Number of words in the term\n","\n","    # Case where there are no nested candidates\n","    if not candidates:\n","        return np.log2(length_a) * freq_a if freq_a > 0 else 0\n","\n","    # Case where nested candidates exist\n","    p_t_a = len(candidates)  # Number of candidates containing the term\n","    sum_f_b = sum(term_frequencies.get(b, 0) for b in candidates)  # Sum of frequencies of candidates\n","    return np.log2(length_a) * (freq_a - (sum_f_b / p_t_a)) if p_t_a != 0 else 0\n","\n","# Calculate C-values for each expression\n","for index, row in df.iterrows():\n","    expression = row['Terms']\n","    freq_a = term_frequencies[expression]  # Frequency of the term\n","    # Identify candidates that contain the expression as a substring\n","    candidates = [exp for exp in term_frequencies.keys() if expression in exp and exp != expression]\n","    # Calculate C-value\n","    c_value = calculate_c_value(expression, freq_a, candidates)\n","    df.at[index, 'C-value'] = c_value  # Store the C-value in the DataFrame\n","\n","# Save the results with C-values to a new Excel file\n","output_file_path = '/content/drive/My Drive/Experiment/statistics_with_cvalues.xlsx'\n","df.to_excel(output_file_path, index=False)\n","\n","print(f\"C-values have been calculated and saved to {output_file_path}\")"],"metadata":{"id":"VXLosv5evbRO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Syntactic analysis of MWTs"],"metadata":{"id":"1PksqCAKvukG"}},{"cell_type":"code","source":["!python -m spacy download en_core_web_lg"],"metadata":{"id":"GvTt1Yq03nG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import spacy\n","import re\n","\n","# Load SpaCy's large model for dependency parsing\n","nlp = spacy.load(\"en_core_web_lg\")\n","\n","# Define the path to your Excel file\n","input_path = '/content/drive/My Drive/Experiment/MWT.xlsx'\n","\n","# Load the data from the Excel file\n","df = pd.read_excel(input_path)\n","\n","# Initialize a new column for syntactic structure representation\n","df['Syntactic structure representation'] = \"\"\n","\n","# Preprocess expression to handle hyphenated words as single tokens\n","def preprocess_expression(expression):\n","    # Replace hyphens within words with underscores to treat them as single tokens\n","    return re.sub(r'(?<=\\w)-(?=\\w)', '_', expression)\n","\n","# Function to extract dependency pairs from a given expression\n","def extract_dependency_pairs(expression):\n","    # Preprocess expression to handle hyphenated terms as single tokens\n","    expression = preprocess_expression(expression)\n","    doc = nlp(expression)\n","    # Extract dependency pairs in the form (token1, token2)\n","    dependency_pairs = [f\"{token.text.replace('_', '-')} {token.head.text.replace('_', '-')}\" for token in doc if token.dep_ != 'ROOT']\n","    # Join pairs as a single string separated by commas\n","    return \", \".join(dependency_pairs)\n","\n","# Process each expression in the \"MWT\" column and store the dependency pairs\n","df['Syntactic structure representation'] = df['MWT'].apply(extract_dependency_pairs)\n","\n","# Save the updated DataFrame back to the Excel file\n","output_path = '/content/drive/My Drive/Experiment/MWT_syntax.xlsx'\n","df.to_excel(output_path, index=False)\n","\n","print(f\"Syntactic structure representation saved to {output_path}\")"],"metadata":{"id":"WCNELCnnvvJZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Statistical analysis of syntactic structure representations"],"metadata":{"id":"caRH2HNFvvme"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"dDX5zDCr350s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Load the SciBERT model and tokenizer\n","model_name = \"allenai/scibert_scivocab_uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name)\n","\n","# Function to compute embeddings with mean pooling\n","def mean_pooling(embeddings, attention_mask):\n","    # Apply mean pooling to get the sentence embedding\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n","    return torch.sum(embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","def get_embedding(text):\n","    # Tokenize and encode the text\n","    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    # Perform mean pooling on the token embeddings\n","    return mean_pooling(outputs.last_hidden_state, inputs['attention_mask']).squeeze()\n","\n","# Load the Excel file and select relevant columns\n","file_path = '/content/drive/My Drive/Experiment/MWT_syntax.xlsx'\n","df = pd.read_excel(file_path, sheet_name='Sheet1')\n","\n","# Initialize a list to store cosine similarities\n","cosine_similarities = []\n","\n","# Iterate over each row to compute cosine similarity between \"Expression\" and \"Syntactic structure representation\"\n","for _, row in df.iterrows():\n","    expression = str(row['MWT'])\n","    structure = str(row['Syntactic structure representation'])\n","\n","    # Get embeddings for each text\n","    expr_embedding = get_embedding(expression)\n","    struct_embedding = get_embedding(structure)\n","\n","    # Calculate cosine similarity and store the result\n","    cos_sim = cosine_similarity(expr_embedding.unsqueeze(0), struct_embedding.unsqueeze(0))[0][0]\n","    cosine_similarities.append(cos_sim)\n","\n","# Add the cosine similarities to a new column \"cosim\"\n","df['cosim'] = cosine_similarities\n","\n","# Calculate and print the mean cosine similarity\n","mean_cosine_similarity = sum(cosine_similarities) / len(cosine_similarities)\n","print(f\"Mean cosine similarity: {mean_cosine_similarity}\")\n","\n","# Save the updated DataFrame back to the Excel file\n","output_file_path = '/content/drive/My Drive/Experiment/MWT_cosine_similarities.xlsx'\n","df.to_excel(output_file_path, index=False)\n","\n","print(\"Cosine similarities calculated and saved in the new file.\")"],"metadata":{"id":"KYh-9nAv3ZLY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Statistical analysis of triplets"],"metadata":{"id":"wu497D9f3Zfb"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Load the SciBERT model and tokenizer\n","model_name = \"allenai/scibert_scivocab_uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name)\n","\n","# Function to compute embeddings with mean pooling\n","def mean_pooling(embeddings, attention_mask):\n","    # Apply mean pooling to get the sentence embedding\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n","    return torch.sum(embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","def get_embedding(text):\n","    # Tokenize and encode the text\n","    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    # Perform mean pooling on the token embeddings\n","    return mean_pooling(outputs.last_hidden_state, inputs['attention_mask']).squeeze()\n","\n","# Load the Excel file and select the \"Triplets\" sheet\n","file_path = '/content/drive/My Drive/Experiment/Triplets.xlsx'\n","df = pd.read_excel(file_path, sheet_name='Sheet1')\n","\n","# Initialize lists to store cosine similarities and differences\n","cosim1_values = []\n","cosim2_values = []\n","differences = []\n","\n","# Iterate over each row to compute cosine similarities\n","for _, row in df.iterrows():\n","    anchor_term = str(row['Anchor term'])\n","    closely_related_term = str(row['Closely related term'])\n","    distantly_related_term = str(row['Distantly related term'])\n","\n","    # Get embeddings for each term\n","    anchor_embedding = get_embedding(anchor_term)\n","    closely_related_embedding = get_embedding(closely_related_term)\n","    distantly_related_embedding = get_embedding(distantly_related_term)\n","\n","    # Calculate cosine similarity between \"Anchor term\" and \"Closely related term\"\n","    cosim1 = cosine_similarity(anchor_embedding.unsqueeze(0), closely_related_embedding.unsqueeze(0))[0][0]\n","    cosim1_values.append(cosim1)\n","\n","    # Calculate cosine similarity between \"Anchor term\" and \"Distantly related term\"\n","    cosim2 = cosine_similarity(anchor_embedding.unsqueeze(0), distantly_related_embedding.unsqueeze(0))[0][0]\n","    cosim2_values.append(cosim2)\n","\n","    # Calculate the difference between cosim1 and cosim2\n","    difference = cosim1 - cosim2\n","    differences.append(difference)\n","\n","# Add the cosine similarities and differences to new columns in the DataFrame\n","df['cosim1'] = cosim1_values\n","df['cosim2'] = cosim2_values\n","df['difference (cosim1-cosim2)'] = differences\n","\n","# Save the updated DataFrame back to the Excel file\n","output_file_path = '/content/drive/My Drive/Experiment/triplet_cosine_similarities.xlsx'\n","df.to_excel(output_file_path, sheet_name='Triplet', index=False)\n","\n","# Print the mean of each calculated column\n","print(f\"Mean cosim1: {sum(cosim1_values) / len(cosim1_values)}\")\n","print(f\"Mean cosim2: {sum(cosim2_values) / len(cosim2_values)}\")\n","print(f\"Mean difference (cosim1 - cosim2): {sum(differences) / len(differences)}\")\n","\n","print(\"Cosine similarities and differences calculated and saved in the new file.\")\n"],"metadata":{"id":"qK9H4XLR3j2A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import scipy.stats as stats\n","from scipy.stats import shapiro\n","\n","# Load the Excel file and select the \"Triplet\" sheet\n","file_path = '/content/drive/My Drive/Experiment/triplet_cosine_similarities.xlsx'\n","df = pd.read_excel(file_path, sheet_name='Triplet')\n","\n","# Extract the columns for cosine similarities and calculate the difference\n","cosim1 = df['cosim1']\n","cosim2 = df['cosim2']\n","cosim_diff = df['difference (cosim1-cosim2)']\n","\n","# Perform the Shapiro-Wilk test for normality on the cosine similarity differences\n","shapiro_stat, shapiro_pvalue = shapiro(cosim_diff)\n","print(\"Shapiro-Wilk Test Statistic:\", shapiro_stat)\n","print(\"Shapiro-Wilk Test p-value:\", shapiro_pvalue)\n","\n","# Check if the data is normally distributed (p-value >= 0.05)\n","if shapiro_pvalue >= 0.05:\n","    print(\"Data is normally distributed. Proceeding with paired t-tests.\")\n","\n","    # Two-tailed paired t-test\n","    t_stat, t_pvalue = stats.ttest_rel(cosim1, cosim2)\n","    print(\"Two-tailed paired T-test Statistic:\", t_stat)\n","    print(\"Two-tailed paired T-test p-value:\", t_pvalue)\n","\n","    # One-tailed paired t-test (alternative hypothesis: cosim1 > cosim2)\n","    t_stat_one_tail, t_pvalue_one_tail = stats.ttest_rel(cosim1, cosim2, alternative='greater')\n","    print(\"One-tailed paired T-test Statistic:\", t_stat_one_tail)\n","    print(\"One-tailed paired T-test p-value:\", t_pvalue_one_tail)\n","\n","else:\n","    print(\"Data is not normally distributed. Proceeding with Wilcoxon signed-rank tests.\")\n","\n","    # Two-tailed Wilcoxon signed-rank test\n","    wilcoxon_stat, wilcoxon_pvalue = stats.wilcoxon(cosim1, cosim2)\n","    print(\"Two-tailed Wilcoxon signed-rank test Statistic:\", wilcoxon_stat)\n","    print(\"Two-tailed Wilcoxon signed-rank test p-value:\", wilcoxon_pvalue)\n","\n","    # One-tailed Wilcoxon signed-rank test (alternative hypothesis: cosim1 > cosim2)\n","    wilcoxon_stat_one_tail, wilcoxon_pvalue_one_tail = stats.wilcoxon(cosim1, cosim2, alternative='greater')\n","    print(\"One-tailed Wilcoxon signed-rank test Statistic:\", wilcoxon_stat_one_tail)\n","    print(\"One-tailed Wilcoxon signed-rank test p-value:\", wilcoxon_pvalue_one_tail)\n"],"metadata":{"id":"2gA_9lvF4U7G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Practical implementation of the EEMWT dataset (example)"],"metadata":{"id":"gEGT6OYY5ewp"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModel\n","import torch\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Load SciBERT model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n","model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n","\n","# Load the positive triplets\n","positive_triplets_df = pd.read_excel('/content/drive/My Drive/Experiment/Triplets.xlsx')\n","\n","# Function to get embeddings using different pooling methods\n","def get_embedding(text, pooling_method='mean'):\n","    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n","    with torch.no_grad():\n","        outputs = model(**inputs).last_hidden_state\n","\n","    # Select the pooling method\n","    if pooling_method == 'mean':\n","        return outputs.mean(dim=1).numpy()  # Mean pooling\n","    elif pooling_method == 'max':\n","        return outputs.max(dim=1).values.numpy()  # Max pooling\n","    elif pooling_method == 'min':\n","        return outputs.min(dim=1).values.numpy()  # Min pooling\n","    elif pooling_method == 'cls':\n","        return outputs[:, 0, :].numpy()  # CLS token pooling\n","    else:\n","        raise ValueError(\"Pooling method must be one of ['mean', 'max', 'min', 'cls']\")\n","\n","# Initialize counters for metrics\n","TP = 0  # True Positive\n","FP = 0  # False Positive\n","total_triplets = len(positive_triplets_df)\n","\n","# Define the pooling methods to evaluate\n","pooling_methods = ['mean', 'max', 'min', 'cls']\n","\n","# Evaluate triplets for each pooling method\n","for pooling_method in pooling_methods:\n","    TP = 0  # Reset TP for each pooling method\n","    FP = 0  # Reset FP for each pooling method\n","\n","    print(f\"\\nEvaluating with {pooling_method} pooling...\")\n","\n","    for _, row in positive_triplets_df.iterrows():\n","        anchor_term = row['Anchor term']\n","        close_term = row['Closely related term']\n","        dist_term = row['Distantly related term']\n","\n","        # Get embeddings using the specified pooling method\n","        anchor_emb = get_embedding(anchor_term, pooling_method)\n","        close_emb = get_embedding(close_term, pooling_method)\n","        dist_emb = get_embedding(dist_term, pooling_method)\n","\n","        # Calculate cosine similarities\n","        sim_close = cosine_similarity(anchor_emb, close_emb)[0][0]\n","        sim_dist = cosine_similarity(anchor_emb, dist_emb)[0][0]\n","\n","        # Determine true positive or false positive\n","        if sim_close > 0.85:  # True Positive threshold\n","            TP += 1  # Correctly identified as closely related\n","        if sim_dist > 0.9:  # False Positive threshold\n","            FP += 1  # Incorrectly identified as closely related\n","\n","    # Calculate false negatives and true negatives\n","    FN = total_triplets - TP\n","    TN = total_triplets - FP\n","\n","    # Calculate metrics\n","    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n","    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n","    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","    accuracy = (TP + TN) / (TP + FP + TN + FN) if (TP + FP + TN + FN) > 0 else 0\n","\n","    # Output results for the current pooling method\n","    print(\"Results:\")\n","    print(f\"  Precision: {precision:.4f}\")\n","    print(f\"  Recall: {recall:.4f}\")\n","    print(f\"  F1-score: {f1:.4f}\")\n","    print(f\"  Accuracy: {accuracy:.4f}\")\n","\n","    # Debugging: Print counts of TP, FP, TN, FN\n","    print(f\"\\nTP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")"],"metadata":{"id":"zb_ei4Z_5kR7"},"execution_count":null,"outputs":[]}]}